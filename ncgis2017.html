<!doctype html>
<html lang="en">
<!-- This is a generated file. Do not edit. -->

    <head>
        <meta charset="utf-8">

        <title>Immersive Tangible Landscape_payam</title>

        <meta name="description" content="Slides for Immersive Tangible Landscape NCGIS 2017 talk">
        <meta name="author" content="NCSU GeoForAll Lab members">

        <meta name="apple-mobile-web-app-capable" content="yes" />
        <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

        <link rel="stylesheet" href="css/reveal.css">
        <link rel="stylesheet" href="css/theme/osgeorel_greyscale.css" id="theme">

        <!-- For syntax highlighting -->
        <link rel="stylesheet" href="lib/css/zenburn.css">
        <!-- For chalkboard plugin -->
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">

        <!-- If the query includes 'print-pdf', include the PDF print sheet -->
        <script>
            if( window.location.search.match( /print-pdf/gi ) ) {
                var link = document.createElement( 'link' );
                link.rel = 'stylesheet';
                link.type = 'text/css';
                link.href = 'css/print/pdf.css';
                document.getElementsByTagName( 'head' )[0].appendChild( link );
            }
        </script>

        <!--[if lt IE 9]>
        <script src="lib/js/html5shiv.js"></script>
        <![endif]-->

        <style>
        body {
        /*background-color: #FFF !important;*/
        /*
          background-image: url("pictures/elevation-nagshead.gif");
          background-repeat: no-repeat;
          background-position: left bottom;*/
        }
        .reveal section img {
            background: transparent;
            border: 0;
            box-shadow: 0 0 0 rgba(0, 0, 0, 0.15);
        }
        /* for standalone frame */
        /*
        iframe {
            display: block;
            margin-left: auto;
            margin-right: auto;
        }
        */
        /* display: inline; background-color: #002B36; padding: 0px; margin: 0px */
        .rounded-corners {
            border: 0px solid black;
            border-radius: 5px;
            -moz-border-radius: 5px;
            -khtml-border-radius: 5px;
            -webkit-border-radius: 5px;
        }
        a:hover {
            color: #444 !important;
            text-decoration: underline !important;
        }
        h1, h2, h3, h4, h5 {
            text-transform: none !important;
            /* word-break: keep-all; text-transform: none; font-size: 200%; line-height: 110%; */
            /* color: #060 !important; */
            /* color: #444 !important; */ /* grey from the wab page */
            font-weight: bold !important;
            -webkit-hyphens: none !important;
            -moz-hyphens: none !important;
            -ms-hyphens: none !important;
            hyphens: none !important;
            line-height: 110% !important;
        }
        .reveal .progress span {
            background-color: #444 !important;
        }
        /* predefined element positioning */
        .top {
            /*position: relative;*/
            top: 5%;
            height: 45%; /* is the height even needed? */
        }
        .bottom {
            height: 45%;
        }
        .ne {
            position: absolute;
            top: 5%;
            right: 5%;
            height: 45%;
            width: 45%;
        }
        .nw {
            position: absolute;
            top: 5%;
            left: 5%;
            height: 45%;
            width: 45%;
        }
        .se {
            position: absolute;
            bottom: 5%;
            right: 5%;
            height: 45%;
            width: 45%;
        }
        .sw {
            position: absolute;
            bottom: 5%;
            left: 5%;
            height: 45%;
            width: 45%;
        }

        /* classes for sections with predefined elements */
        /* using !important because, reveal styles are applied afterwards  */
        .right, .textimg > img, .textimg > video, .textimg > iframe, .imgtext > p, .imgtext > ul, .imgtext > ol, .imgtext > div {
            float: right;
            text-align: left;
            max-width: 47% !important;
        }
        .left, .imgtext > img, .imgtext > video, imgtext > iframe, .textimg > p, .textimg > ul, .textimg > ol, .textimg > div {
            float: left;
            text-align: left;
            max-width: 47% !important;
        }
        li > ul, li > ol {
            font-size: 85% !important;
            line-height: 110% !important;
        }
        .small {
            font-size: smaller !important;
            color: gray;
            margin: 0.1em !important;
        }
        .credit {
            font-size: small !important;
            color: gray;
            margin: 0.1em !important;
        }
        </style>
    </head>

    <body>

        <div class="reveal">

            <!-- Any section element inside of this container is displayed as a slide -->
            <div class="slides">
<section>
<h3 style="color: #888">
    NCGIS 2017</h3>
<h1 style="margin-top: 0.5em; margin-bottom: 0em; color: #000">Immersive Tangible Landscape</h1>
<h5 style="color: #888">Payam Tabrizian, Anna Petrasova, Brendan Harmon, Vaclav Petras, Helena Mitasova</h5>

<img height="80px" style="margin-top: 2em" src="img/cgaBlack.png">
<h5 style="color: #000"> North Carolina State University</h5>

 <aside class="notes">
    Good afternoon, my name is Anna and together with Payam we will talk today
    about Immersive Tangible Landscape,
 </aside>
</section>


<section>
<img height="100px" style="margin-top: 2em; margin-bottom: 1em" src="img/cgaBlack.png">
<p style="margin-top: 1em;">
The talk is presented by the <b>GeoForAll Laboratory</b> at the
Center for Geospatial Analytics (CGA), North Carolina State University
<p style="margin-top: 1em;">
CGA is an interdisciplinary research and education center with focus on
geospatial computing, modeling, analytics and geovisualization.
<p style="margin-top: 1em;">
<p style="font-size: 140%; margin-top: 1em;">
<a href="http://geospatial.ncsu.edu/">geospatial.ncsu.edu</a>

<aside class="notes">
   which is a project we developed as members of GeoForAll laboratory
   to make landscape design process more effective through the use of Tangible
   interaction, Immersive virtual environments, and geospatial analytics.
   Our lab is part of the Center for Geospatial Analytics, which focuses
   on educationa nd interdisciplinary research, and has been instrumental in
   making this research happen.
</aside>
</section>

<section>
<img class="stretch" src="img/IVE+TL.jpg">
<div style="margin-left:7%;float:left;max-width:45% !important">
Tangible Landscape</div>
<div style="margin-right:5%;float:right;max-width:45% !important"> Immersive Virtual Environment (IVE)</div>

<aside class="notes">
    In the first part of this talk, I will give you an overview of Tangible Landscape,
    a tangible interface for GIS, which we have been developing and applying for couple years now, 
    and Payam will then show you the newest coupling of TL with Immersive virtual environments,
    and explain how this coupling enhances our current design approaches.
</aside>
</section>
                
                
<section>
<h2>Motivation for Tangible Interfaces for GIS</h2>
<ul>
    <!-- <li>Tangible User Interfaces (TUI) can helps us experience and better understand data and processes through tangible interaction.</li> -->
    <li>Interaction through mouse, keyboard and display does not encourage creativity.</li>
    <li>Manipulating computer models is not intuitive and requires specialized software and training.</li>
    <li>Collaboration is restricted as typically only one user at a time can navigate and modify models. </li>

<aside class="notes">
    So why are we interested in Tangible interfaces?
    I am sure this photo shows a familiar setting -
we often get together around a screen to solve a geospatial problem or use mouse or touch 
to manipulate 3D data on 2D screen. Such manipulation of data often requires knowledge of a specific,
often complex software, usually only single person can access the data 
creating barriers to collaboration and creativity. 
</aside>
</ul>

<img height="250px" src="img/collaboration_computer.JPG">
<img height="250px" src="img/art_rhino.jpg">
<!-- </br></br></br> -->
<!-- <h5>Tangible Landscape is designed to make scientific data,
   models, and simulations exploratory, engaging, and fun</h5> -->
</section>


<section>
<h2>Tangible interface linked with GIS</h2>
<img height="250px" src="img/tangeoms_5_s.jpg">
<img height="250px" src="img/tangeoms_2.jpg">
<p><b>Tangible Geospatial Modeling System (TanGeoMS)</b></p>
<p><small>L. Tateosian, H. Mitasova, B. A. Harmon, B. Fogleman, K. Weaver, and R. S. Harmon, <a href="http://baharmon.github.io/publications/tangible_geospatial_modeling.pdf">“TanGeoMS: tangible geospatial modeling system.,”</a>
     IEEE Trans. Vis. Comput. Graph., vol. 16, no. 6, pp. 1605–12, 2010.</small></p>
     
<aside class="notes">
    More than 10 years ago, researchers at the MIT Media Lab tried
     to adddress this issue and developed early prototypes
of environments that coupled 3D physical models with analysis.
My advisor Dr. Helena Mitasova developed a similar system, but linked to GIS
 to support modeling and analysis 
of real world landscapes based on GIS data. The system used laboratory lidar and it was very expensive,
limiting its broader use.
</aside>
</section>

<section>
<h2>Kinect-based systems</h2>
<img class="stretch" src="img/augmented_reality_sandbox.jpg">
<p><b>Augmented Reality Sandbox by KeckCAVES</b>
<p style="font-size:80%">Expensive laser scanners replaced by low cost Kinect
<p><small>Image source:
     <a href="http://idav.ucdavis.edu/~okreylos/ResDev/SARndbox/">http://idav.ucdavis.edu/</a></small></p>

<aside class="notes">
    The breakthrough for this approach came with a new generation of low cost 3D sensors,
    specifically Kinect. Several systems were developed using Kinect,
     the best known among them
    is probably the Augumented sandbox by Keckcaves developed at UC Davis.
    You might have seen this systems at museums or conferences where it often generates a lot of excitement.

</aside>
</section>

<!-- Near real time interaction -->
<section>
<h2>Tangible Landscape: real-time coupling with GIS</h2>
<iframe data-autoplay width="853" height="480" src="https://www.youtube.com/embed/Cd3cCQTGer4?rel=0&amp;showinfo=0&amp;loop=1&amp;playlist=Cd3cCQTGer4" frameborder="0" allowfullscreen></iframe>
<p>With Tangible Landscape you can hold a GIS in your hands - feeling the shape of the earth, sculpting its topography, and directing the flow of water.</p>

<aside class="notes">
    We were able to take advatage of the fast and relatively accurate 3D scanning by Kinect
and developed the first system with real-time coupling of a 3D physical model, with GIS.
This video should give you a basic idea of the interaction - using a model of a real landscape,
we can modify the topography and get instant feedback on how our changes impact water flow and ponding.
</aside>
</section>

<!-- How it works -->
<section>
<h2>How it works</h2>
<img class="stretch" src="img/system_schema.png">
<p>Tangible Landscape couples a digital and a physical model through a continuous cycle of 3D scanning, geospatial modeling, and projection</p>
<aside class="notes">
    So how does the system work? In the previous slide you have seen the 3D model of a landscape.
This model is continuously scanned by the kinect, the scanned data are imported into GRASS GIS,
where a 3D digital elevation model is computed and a selected analysis or modeling is performed -
in our case contours are derived and water flow and ponding is simulated. A composite image
of the selected map layers is then projected over the model. In this way the system couples
the digital and physical models in a continuous cycle of scanning, modeling and projection,
providing the user continuous feedback.
</aside>
</section>

<!-- Software -->
<section>
<h2>Software</h2>
<img class="stretch" src="img/software-schema.png">
<aside class="notes">
    So let’s have a look at the software behind Tangible Landscape. Tangible Landscape is built around GRASS GIS platform. GRASS GIS is an open source, multiplatform GIS offering a variety of simple to complex tools for geospatial analysis, but also remote sensing, network analysis or hydrology.
Tangible Landscape has 3 main components. First we have a GRASS GIS add-on module r.in.kinect, which continuously receives point cloud from Kinect and processes this data into a digital elevation model.
Then we have Tangible Landscape plugin integrated into GRASS GIS graphical user interface which serves for controlling Tangible Landscape, specifically the scanning parameters and timing.
The third component is a Python file with geospatial analyses organized in functions called for each new scan. We developed a library of functions you can use right away, but you can also develop your own geospatial workflows using GRASS GIS Python API.
</aside>
</section>

<!-- Equipment list and budget -->
<section>
<h2>System cost</h2>
<table>
<tr><th>Type</th><th>Product example</th><th>Cost</th></tr>
<tr><td>Software</td><td>Tangible Landscape plugin for GRASS GIS</td><td>$0</td></tr>
<tr><td>Computer</td><td>System76 Oryx Pro</td><td>$1500</td></tr>
<tr><td>Projector</td><td>Optoma ML750 WXGA LED</td><td>$500</td></tr>
<tr><td>3D sensor</td><td>Xbox One Kinect</td><td>$100</td></tr>
<tr><td></td><td>Kinect Adapter for Windows</td><td>$50</td></tr>
<tr><td>Stand</td><td>2 x Avenger 40-Inch C-Stand with Grip Kit</td><td>$400</td></tr>
<tr><td></td><td>2 x Avenger 3-Inch Baby Wall Plate</td><td>$20</td></tr>
<tr><td>Peripherals</td><td>HDMI cable, extension cord</td><td>$20</td></tr>
<tr><td>Modeling media</td><td>Waba Fun Kinetic Sand 11 Lbs</td><td>$50</td></tr>
<tr style="border-top: 1px solid #383838"><td>Total</td><td></td><td>$2640</td></tr>
</table>
</section>






<section>
<h2>Interactions</h2>
<img class="stretch" src="img/interactions.png">
<table width="100%">
        <col width="16%">
        <col width="18%">
        <col width="18%">
        <col width="18%">
        <col width="18%">
        <tr>
            <td style="vertical-align: middle; text-align:center; border-bottom: 0px; padding: 0;">surface
            <td style="vertical-align: middle; text-align:center; border-bottom: 0px; padding: 0;">points
            <td style="vertical-align: middle; text-align:center; border-bottom: 0px; padding: 0;">lines
            <td style="vertical-align: middle; text-align:center; border-bottom: 0px; padding: 0;" >areas
            <td style="vertical-align: middle; text-align:center; border-bottom: 0px; padding: 0;" >areas
        </tr>
</table>
<aside class="notes">
You have so far seen only sculpting sand with our hands, where we modify the continuous elevation surface. However some applications require different types of input data, such as objects. To make Tangible Landscape flexible in this regard, we developed multiple ways to interact with the physical models. Here we use a wooden marker to specify point locations on the landscape, for example view points or trailheads. Recently we have started to experiment with using laser pointer to draw objects, such as points, lines or polygons. Another option is to use colored sand to create polygons where the color represents certain attribute of the polygon and the height of the sand can represent intensity of that property. The most recent interaction we are testing now is creating areas using colored felt or paper of different shapes placed on the model. 
These interactions can be combined to achieve intuitive interactions for particular application. Now I will show you some of the applications we developed for different study sites, using different geospatial models and each of them has different type of interaction.
</aside>
</section>



<!-- Science with Tangible Landscape -->

<!-- Visibility analysis -->
<section>
<h2>Applications: visibility</h2>
<video  data-autoplay class="stretch" controls>
<source src="img/visibility.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
<p>Visibility analysis
    <aside class="notes">
        Topography is directly linked to visibility, so here we explore viewsheds at our campus. The physical model from sand represents digital surface model with canopy and we place the markers to specify viewpoints. Once the marker is detected, the viewshed is dynamically computed and visualized, here the visible areas are represented by yellow color.
    </aside>
</section>

<!-- Futures -->
<section>
<h2>Applications: urban growth</h2>
<video  data-autoplay class="stretch" controls muted>
<source src="video/futures.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
<p>Simulation of urban growth scenarios with FUTURES model
    <aside class="notes">
        Here we switch from disease spread to urbanization application. We coupled TL with urban growth model called FUTURES implemented as a GRASS add-on and developed here at North Carolina State University. By placing colored sand we create red zones which attract new development or green zones for conservation. The height of the sand can represent the intensity - how much the zone attracts the development. Then we identify the polygons and rerun the urban growth model with these new conditions. 
Now you can observe the animated growth of the city as predicted by the FUTURES based on the specified interventions.
    </aside>
</section>


<!-- Serious gaming with Tangible Landscape: Coastal -->
<section>
<h2>Serious games: coastal flooding</h2>
<img width="32%" src="img/tl_coastal_1s.png">
<img width="32%" src="img/tl_coastal_2s.png">
<!--<img width="22%" src="img/tl_coastal_3s.png">-->
<img width="32%" src="img/tl_coastal_4s.png">
<p>Save houses from coastal flooding by building coastal defenses</p>
<p style="font-size:0.75em">Structured problem-solving with rules, challenging objectives, and scoring</p>
<aside class="notes">
    Recently there has been a lot of excitement about serious games and how we can use them to engage public in science. We thought Tangible Landscape would be a great tool for serious gaming, so let’s look at a coastal flooding game. We prepared this game for a public event and people playing the game were trying to protect the homes on the coast when a foredune is breached during a storm surge. With limited sand budget they tried different ways of building barriers and they learned pretty quickly that a breach in one place can cause flooding of houses which are far away from the breach.
</aside>
</section>


<!-- IVE COUPLING SLIDES "> -->

<!-- motivation for coupling -->

<section> 
<h2> Coupling Tangible Landscape with IVE </h2>
<ul>
    <li> Better communicating the implications of landscape change </li>
    <li> Including design attributes in landscape planning process </li>
    <li> Assessing trade-offs between ecological and experiential quality (e.g., preferences, pyschological well-being) </li>
</ul>
<img class="stretch" src="img/flooding_secraf.jpg">
 
    <aside class="notes">
    
    As you have seen so far, Tangible Landscape represents the landscape as a projection-augmented model which is perceived in a bird’s-eye perspective. We aimed to complete the picture by representing the landscape similar to how we perceive in human-scale. 
    So why it is important to include human perception ? 

    First, this allows for a more tangible understanding and communicating the implications of landscape change that are important components in decision making and stake-holder participation.  What it means if some areas is flooded ? or how your living environment looks like after some restoration intervention ? 
    Second, it allows bringing designers into the table and include attributes that they care about, like composition of landscape, coherence and etc. 
    Third, given our growing understanding about the impact of landscapes on individual’s mental and physical health , it is is imperative to find those sweet spots where the ecological functioning and human-perception measures such as aesthetic evaluation and landscape preferences are balanced. 

    </aside>   
  
</section> 

<!-- IVE -->
<section> 
<h2> Immersive Virtual Environments (IVE) </h2>
<ul>
<li>Immersive Virtual Environements surround the user in images, video or other stimuli to generate a perception of being physically present in a non-physical world.</li> 
<li> High degree of "presence", more robust asessement of human perception and preferences </li>

</ul>
<img class="stretch" src="img/IVE.jpg">
<p style="font-size:0.5em"> http://marclee.io/en/10-000-moving-cities-same-but-different-vr/ </p>
    
    <aside class="notes">
    IVE’s surround user with continous stream of stimuli, tied to the users head or body movement , creating a feeling being physically present in a virtual world. 
    They are shown to elicit a high degree of presence and immersion, and very robust tools for assessing perceptions. 
    </aside>  

</section> 

<!-- Coupling -->
<section>
<h2> The coupling rationale </h2>
<ul>
<li> Real-time updating a georeferenced 3D model of the landscape based on user interaction with Tangible Landscape </li>
<li> Updating the attributes (shape, position) of 3D objects (e.g., plants) and surfaces (e.g., terrain) with their corresponding tangible objects  </li>
<li> Enabling user to control the viewpoints (camera position) and animation (e.g., walkthrough, flythrough) </li>
</ul>
    <aside class="notes">
    The coupling concept is based on adaptive 3d modelling framework. The idea was to generate a georeferenced 3D world of the under-study landscape, in which all the features and behavior of 3D elements like trees, buildings and surfaces are linked to their corresponding tangible object in tangible landscape. In this way, as users manipulate the tangible model and pieces, they can see, in real time, the changing landscape rendered on display or through virtual reality headsets like oculus. 
    In addition to the automation adaptation aspect, we wanted to allow users to control the camera and animation so they can step into and navigate in their desired location in the landscape. 
    </aside>  
</section>

<!-- physical setup -->
<section>
<h2>Physical setup</h2>
    
<img class="stretch" src="img/blender.png">
    <aside class="notes">
    For implementing the concept, we added a 3D modeling and game engine software, called blender, to the tangible landscape setup with outputs to a display and an immersive virtual reality headset.
    </aside>  
</section>

<!-- What is Blender ? Why Blender ? -->
<section>

<h2> What is Blender? Why Blender? </h2>
<ul>
    <li>Free and open source 3d modelling and game engine software </li>
    <li>Easy scripting (Python) </li>
    <li>GIS and Virtual reality plugin</li>
    <li>High-quality real-time rendering and shading </li>  
</ul>
<img class="stretch" src="img/blender_sample.jpg">
<!-- <p style="font-size:0.5em"> agiro.cgsociety.org </p>-->
    <aside class="notes">
    Blender is a free and open source program for modeling, rendering, simulation, animation, and game design. The software has an internal python-based IDE and add-ons for importing GIS data to georeference the scene, and displaying the viewport in HMDs. It also allows realtime high-quality rendering and shading. 
    </aside>  
</section>

<!-- Software Architecture -->
<section>
<h2> Software Architecture </h2>
<img class="stretch" src="img/blender_workflow-01.jpg">
</section>

<!-- Features -->
<section class="textimg">
    <div style="text-align:left">
        <p> <b>Interaction:</b> hand </p>
        <p> <b>3D processing:</b> terrain GeoTIFF raster and water polygon </p>
        <p> <b>Simulation:</b> simple hydrologic model </p>
    </div>
<img src="img/feature1.jpg">
    
    <aside class="notes">
    Briefly describing the workflow, GRASS GIS and Blender are loosely coupled through file-based communication. As user interacts with the tangible model or objects, GRASS GIS sends a copy of the geo-coordinated information or simulation to a specified system directory. 
    We implemented a monitoring module in blender scripting environment that constantly watches the directory, identifies the type of incoming information, and apply relevant operations needed to update the 3d model. The input data can range from geospatial features like a raster or a point cloud, simple coordinates as a text file, or signals that prompt a command such as removing an object from the scene. 
    </aside>  
    
</section>

<!-- Features -->
<section>

<img class="stretch" src="img/feature2.jpg">
<div style="text-align:left">
<p> <b>Interaction:</b> laser pointer </p>
<p> <b>3D processing:</b> importing line feature to update camera position </p>
    <aside class="notes">
    For example, when landscape is manipulated with hand, a geotiff raster and a polygon related to water is processed.
    </aside>
</div>
</section>

<!-- Vantage Points -->
<section>
<img class="stretch" src="img/features_6.jpg">
<div style="text-align:left">
<p> <b>Interaction:</b> laser pointer </p>
<p> <b>3D processing:</b> importing polygon features to populate trees </p>
</div>
  <aside class="notes">
  While anytime during the interaction user can freely navigate in the environment using the mouse, they can also use a laser pointer to delineate their prefered vantage point. 
  Lines created with laser pointer can be transferred as a line feature denoting user’s desired viewpoint and direction of view. The scene camera is then relocated to the line’s starting point and the direction of view is aligned to the line’s endpoint.
  </aside>
      </section>

<!-- Planting -->
<section>
<img class="stretch" src="img/features_7.jpg">
<div style="text-align:left">
<p> Experiencing the landscape through IVE </p>
<p> <b>Interaction:</b> laser pointer </p>
</div>
    <aside class="notes">
    Aslo, Polygon features drawn with laser-pointer can be defined as patches of trees to populate predefined diversity and density of vegetation. 
    </aside>
</section>

<!-- IVE -->
<section>
    <aside class="notes">
    The viewport is continuously displayed in both viewport and headmounted display, so users can pick up the headset and get immersed in their prefered views.
    </aside>     
</section>

<!-- Video -->                
<section>

   <iframe data-autoplay class="stretch"  src="https://www.youtube.com/embed/pYbpEMjME1Y?rel=0&amp;showinfo=0" frameborder="0" allowfullscreen></iframe>
   <aside class="notes">
      For demonstration of the application in action, we will go through a small video of our first prototype where our colleague, Vatslav, and Anna collaboratively design a landscape. Through the design process, please note that how the developments enables the dialogue between ecological assessment and aesthetic evaluation. 
      Ponding and Landscape design
        Vatslav starts by sculpting the topography to create retention ponds to address stormwater management requirements of the site .  As as he carves the landscape, Water flow and accumulation simulations are continuously projected onto the physical model. In the same time, point-cloud and water polygon is transferred to update the tangible model. 
        Then he collaboratively works with anna to place the excavated soil on-site to create artificial mounds that provide overviews to the site.  
        For showing human-scale views we took benefit of the laser pointer interaction. You can delineate the desired view-point and orientation in the landscape which is processed as a line feature as sent to blender to update the camera location and orientation.  Additionally, Users can explore the landscape with the mouse or with the immersive headset.
      Vegetation design.
        Aslo, Polygon features delineated with laser-pointer can be defined as patches of trees to populate predefined diversity and density of vegetation. In this demonstration, only one type of tree is used. However, the blender script can be adjusted to detect various types of plant species. In our current current revisions we have added shading and textures  improve the realism of the scene. 
      Trail Design 
        Tangible objects are also processed in the application. For instance here, wooden cubes represent checkpoints that denote a recreational trail. Grass GIS, simulates the optimal route using an algorithm that simulates the least cost walking path. But Vatslav want to complement the trail experience and adjusts it to meander within the new forested patches.
        The trail line feature not only represent the trail but also processed in Blender as a walktrough simulation that can viewed on screen or in HMD. 

</section>

<!-- Video -->
<section>
<h2> Future work </h2>
<ul>
<li> Landscape planting design </li>
<li> Habitat connectivity and landscape preferences in woodland patches </li>
</ul>
<img class="stretch"  src="img/system_schema_planting.jpg">

</section>


<!-- Open source -->
<section>
<h2>Open source</h2>
<!--<p><a href="https://github.com/baharmon/tangible_topography">Repository with experiment instructions, scripts, data, and results</a></p>-->
<p>Tangible Landscape plugin for GRASS GIS <br>
    <a href="https://github.com/tangible-landscape/grass-tangible-landscape">
        github.com/tangible-landscape/grass-tangible-landscape
    </a></p>
<p>GRASS GIS module for importing data from Kinect v2 <br>
    <a href="https://github.com/tangible-landscape/r.in.kinect">
        github.com/tangible-landscape/r.in.kinect
    </a></p>
<p>Tangible Landscape repository on Open Science Framework <br>
    <a href="https://osf.io/w8nr6/">
        osf.io/w8nr6
    </a></p>
<img width="20%" src="img/tl_logo.png">
</section>


<!-- Book -->
<section>
<h3>Resources</h3>
<!-- website, open education paper, book -->
<ul>
    <li>Tangible Landscape website:  <a href="https://tangible-landscape.github.io">tangible-landscape.github.io</a></li>
    <li>Tangible Landscape wiki: <br><a href="https://github.com/tangible-landscape/grass-tangible-landscape/wiki">github.com/tangible-landscape/grass-tangible-landscape/wiki</a> </li>
    <li>Book: <a href="http://www.springer.com/us/book/9783319257730">
        <em>Tangible Modeling with Open Source GIS</em></a></li>
    <!-- <li><em><a href="http://www.mdpi.com/2220-9964/4/2/942/pdf">
        Integrating Free and Open Source Solutions into Geospatial Science Education.</a></em>
        Petras, V., Petrasova, A., Harmon, B., Meentemeyer, R.K., Mitasova, H.
         ISPRS IJGI. 2015.</li> -->
</ul>
<!-- <img width="20%" src="img/tl_book_cover.png"> -->
</section>

<!-- This is a generated file. Do not edit. -->
        </div>  <!-- slides -->

    </div>  <!-- reveal -->

        <script src="lib/js/head.min.js"></script>
        <script src="js/reveal.js"></script>

        <script>

            // Full list of configuration options available here:
            // https://github.com/hakimel/reveal.js#configuration
            Reveal.initialize({
                // Display controls in the bottom right corner
                controls: false,

                // Display a presentation progress bar
                progress: true,
                
                center: true,
                
                // Display the page number of the current slide
                slideNumber: false,

                // Enable the slide overview mode
                overview: true,

                // Turns fragments on and off globally
                fragments: true,

                // The "normal" size of the presentation, aspect ratio will be preserved
                // when the presentation is scaled to fit different resolutions. Can be
                // specified using percentage units.
                 width: 1060,
                // height: 700,
                
                // Factor of the display size that should remain empty around the content
                margin: 0.05,  // increase?

                // Bounds for smallest/largest possible scale to apply to content
                minScale: 0.5,
                maxScale: 5.0,

                theme: Reveal.getQueryHash().theme,  // available themes are in /css/theme
                transition: Reveal.getQueryHash().transition || 'none', // default/cube/page/concave/zoom/linear/fade/none

                // Push each slide change to the browser history
                history: true,
                // Enable keyboard shortcuts for navigation
                keyboard: true,

                // Vertical centering of slides
                center: true,

                // Enables touch navigation on devices with touch input
                touch: true,

                // Loop the presentation
                loop: false,
                // Flags if the presentation is running in an embedded mode,
                // i.e. contained within a limited portion of the screen
                embedded: false,

                // Number of milliseconds between automatically proceeding to the
                // next slide, disabled when set to 0, this value can be overwritten
                // by using a data-autoslide attribute on your slides
                autoSlide: 0,

                // Stop auto-sliding after user input
                autoSlideStoppable: true,

                // Enable slide navigation via mouse wheel
                mouseWheel: false,

                // Hides the address bar on mobile devices
                hideAddressBar: true,

                // Opens links in an iframe preview overlay
                previewLinks: false,

                // Transition speed
                transitionSpeed: 'default', // default/fast/slow

                // Transition style for full page slide backgrounds
                backgroundTransition: 'none', // default/none/slide/concave/convex/zoom

                // Number of slides away from the current that are visible
                viewDistance: 3,

                // Parallax background image
                //parallaxBackgroundImage: '', // e.g. "'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'"

                // Parallax background size
                //parallaxBackgroundSize: '' // CSS syntax, e.g. "2100px 900px"
                // Optional libraries used to extend on reveal.js
                dependencies: [
                    { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
                    { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                    { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                    { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
                    { src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
                    { src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },
                    { src: 'plugin/math/math.js', async: true }
                ]
            });

        </script>

    </body>
</html>

